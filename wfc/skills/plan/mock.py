"""
wfc-plan Mock - Generate sample plan files for testing wfc-implement

ELEGANT: Simple mock, does one thing well
"""

from pathlib import Path

MOCK_TASKS = """# Implementation Tasks

Generated by wfc-plan (MOCK)

## TASK-001: Setup Project Structure

- **ID**: TASK-001
- **Title**: Setup Project Structure
- **Complexity**: S
- **Dependencies**: []
- **Properties Satisfied**: []
- **Requirements**: []
- **Files Likely Affected**: [`setup.py`, `README.md`, `src/__init__.py`]

**Description**:
Create initial project directories and basic configuration files.

**Acceptance Criteria**:
1. `src/` directory exists with `__init__.py`
2. `tests/` directory exists
3. `README.md` exists with project description
4. `setup.py` or `pyproject.toml` exists

---

## TASK-002: Implement Core Logic

- **ID**: TASK-002
- **Title**: Implement Core Logic
- **Complexity**: M
- **Dependencies**: [TASK-001]
- **Properties Satisfied**: [PROP-001, PROP-002]
- **Requirements**: [FR-001]
- **Files Likely Affected**: [`src/core.py`, `src/utils.py`]

**Description**:
Implement the main business logic for the core functionality.

**Acceptance Criteria**:
1. Core function implemented and documented
2. Returns expected output for valid input
3. Handles edge cases gracefully
4. Follows ELEGANT principles (simple, clear, testable)

---

## TASK-003: Add Comprehensive Tests

- **ID**: TASK-003
- **Title**: Add Comprehensive Tests
- **Complexity**: S
- **Dependencies**: [TASK-002]
- **Properties Satisfied**: [PROP-001, PROP-002]
- **Requirements**: []
- **Files Likely Affected**: [`tests/test_core.py`, `tests/test_utils.py`]

**Description**:
Add test coverage for all core functionality.

**Acceptance Criteria**:
1. All core functions have unit tests
2. Edge cases are tested
3. Property-based tests for PROP-001 and PROP-002
4. Test coverage >= 90%
"""

MOCK_PROPERTIES = """# Formal Properties

Generated by wfc-plan (MOCK)

## PROP-001: Correctness

- **ID**: PROP-001
- **Type**: INVARIANT
- **Statement**: Core function always returns valid output for valid input
- **Rationale**: Ensure reliability and correctness of core functionality

**Formal Specification**:
```
∀ valid_input: core_function(valid_input) → valid_output
∀ invalid_input: core_function(invalid_input) → Error
```

**Acceptance Criteria**:
1. Valid input always produces valid output
2. Invalid input produces clear error message
3. No silent failures or undefined behavior

**Linked Requirements**: [FR-001]
**Linked Tests**: [TC-001, TC-002]
**Linked Tasks**: [TASK-002, TASK-003]

---

## PROP-002: Performance

- **ID**: PROP-002
- **Type**: PERFORMANCE
- **Statement**: Core function completes within 100ms for typical input
- **Rationale**: Ensure acceptable performance for user experience

**Formal Specification**:
```
∀ typical_input: duration(core_function(typical_input)) < 100ms
```

**Acceptance Criteria**:
1. Typical input processed in < 100ms
2. Large input processed in < 1s
3. No memory leaks or unbounded growth

**Linked Requirements**: [NFR-001]
**Linked Tests**: [TC-003]
**Linked Tasks**: [TASK-002]
"""

MOCK_TEST_PLAN = """# Test Plan

Generated by wfc-plan (MOCK)

## Test Strategy

### Unit Tests
- Test all core functions in isolation
- Mock external dependencies
- Fast execution (< 1s total)

### Integration Tests
- Test end-to-end flows
- Real dependencies where possible
- Acceptable execution time (< 10s total)

### Property-Based Tests
- Use hypothesis or similar for property testing
- Focus on PROP-001 and PROP-002

## Test Cases

### TC-001: Core Function with Valid Input

- **ID**: TC-001
- **Property**: PROP-001
- **Type**: Unit
- **Priority**: HIGH

**Setup**:
```python
from src.core import core_function
```

**Input**: Valid test data

**Expected Output**: Valid result matching specification

**Assertions**:
- Output type is correct
- Output value is valid
- No exceptions raised

---

### TC-002: Core Function with Invalid Input

- **ID**: TC-002
- **Property**: PROP-001
- **Type**: Unit
- **Priority**: HIGH

**Setup**:
```python
from src.core import core_function
import pytest
```

**Input**: Invalid test data

**Expected Output**: Clear error message

**Assertions**:
- Appropriate exception raised
- Error message is descriptive
- No silent failure

---

### TC-003: Core Function Performance

- **ID**: TC-003
- **Property**: PROP-002
- **Type**: Performance
- **Priority**: MEDIUM

**Setup**:
```python
from src.core import core_function
import time
```

**Input**: Typical workload

**Expected Output**: Completes within 100ms

**Assertions**:
- Duration < 100ms
- Result is correct
- No performance degradation over multiple runs
"""


def generate_mock_plan(output_dir: Path, project_name: str = "Mock Project") -> None:
    """
    Generate mock plan files for testing.

    Args:
        output_dir: Directory to write mock files
        project_name: Optional project name
    """
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Write files
    (output_dir / "TASKS.md").write_text(MOCK_TASKS)
    (output_dir / "PROPERTIES.md").write_text(MOCK_PROPERTIES)
    (output_dir / "TEST-PLAN.md").write_text(MOCK_TEST_PLAN)

    print(f"✅ Mock plan files generated in {output_dir}")
    print("   - TASKS.md (3 tasks)")
    print("   - PROPERTIES.md (2 properties)")
    print("   - TEST-PLAN.md (3 test cases)")


if __name__ == "__main__":
    # Test mock generation
    import tempfile

    with tempfile.TemporaryDirectory() as tmpdir:
        generate_mock_plan(Path(tmpdir) / "plan")
        print("\n✅ Mock generation test passed")

# PLANNING.md

**Architecture, Design Principles, and Absolute Rules for WFC**

> This document is read by Claude Code at session start to ensure consistent, high-quality development aligned with project standards.

---

## üéØ Project Vision

**WFC (World Fucking Class)** transforms multi-agent code review through:
- **99% token reduction** via ultra-minimal prompts + file references
- **Agent Skills compliance** for Claude Code integration
- **56 expert personas** with automatic selection
- **Weighted consensus** algorithm for quality decisions

**Core Mission**: Provide production-grade, token-efficient, multi-agent consensus code review that is:
- **Accurate** - Specialized personas with domain expertise
- **Efficient** - 99% token reduction (150k ‚Üí 1.5k tokens)
- **Compliant** - Agent Skills specification enforced
- **Validated** - Automated checks prevent regressions

---

## üèóÔ∏è Architecture Overview

### Current State (v0.1.0)

WFC is a **Python package** with:
- Ultra-minimal persona system (200 tokens per persona)
- File reference architecture (send paths, not content)
- Token budget manager (accurate tiktoken counting)
- Consensus algorithm (weighted voting)
- 17 Agent Skills compliant skills
- Professional CLI (`wfc` command)
- Automated validation (pre-commit + CI/CD)

```
WFC Architecture v0.1.0
‚îÇ
‚îú‚îÄ‚îÄ Core Package (wfc/)
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                    # Executable code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ personas/               # Persona system
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ token_manager.py            # 99% token reduction
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ultra_minimal_prompts.py    # 200-token prompts
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ file_reference_prompts.py   # File refs not content
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ persona_executor.py         # Prepare subagent tasks
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ persona_orchestrator.py     # Select personas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/                   # Hook infrastructure
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pretooluse_hook.py          # PreToolUse hook handler
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ security_hook.py            # Security enforcement
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rule_engine.py              # Custom rule engine
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config_loader.py            # Hook configuration
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hook_state.py               # Hook state management
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ patterns/                   # Security patterns (JSON)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ skills/                 # Skill implementations
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ review/
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ orchestrator.py         # Review workflow
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ consensus.py            # Consensus algorithm
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ agents.py               # Agent logic
‚îÇ   ‚îú‚îÄ‚îÄ references/                 # Progressive disclosure
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ personas/               # 56 expert personas (JSON)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TOKEN_MANAGEMENT.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ULTRA_MINIMAL_RESULTS.md
‚îÇ   ‚îî‚îÄ‚îÄ assets/                     # Templates, configs
‚îÇ       ‚îî‚îÄ‚îÄ templates/
‚îÇ           ‚îî‚îÄ‚îÄ playground/         # HTML playground templates
‚îÇ
‚îú‚îÄ‚îÄ Installed Skills (~/.claude/skills/wfc-*)
‚îÇ   ‚îú‚îÄ‚îÄ wfc-review/                 # Multi-agent consensus review
‚îÇ   ‚îú‚îÄ‚îÄ wfc-plan/                   # Adaptive planning
‚îÇ   ‚îú‚îÄ‚îÄ wfc-implement/              # Parallel implementation
‚îÇ   ‚îú‚îÄ‚îÄ wfc-security/               # STRIDE threat analysis
‚îÇ   ‚îú‚îÄ‚îÄ wfc-architecture/           # Architecture docs + C4 diagrams
‚îÇ   ‚îú‚îÄ‚îÄ wfc-test/                   # Property-based tests
‚îÇ   ‚îú‚îÄ‚îÄ wfc-safeguard/              # Real-time security enforcement hooks
‚îÇ   ‚îú‚îÄ‚îÄ wfc-rules/                  # Markdown-based custom enforcement rules
‚îÇ   ‚îú‚îÄ‚îÄ wfc-playground/             # Interactive HTML playground generator
‚îÇ   ‚îî‚îÄ‚îÄ ... (17 total)
‚îÇ
‚îú‚îÄ‚îÄ Tests (tests/)
‚îÇ   ‚îú‚îÄ‚îÄ test_implement_e2e.py       # End-to-end tests
‚îÇ   ‚îú‚îÄ‚îÄ personas/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_persona_selection.py
‚îÇ   ‚îî‚îÄ‚îÄ skills/
‚îÇ       ‚îî‚îÄ‚îÄ plan/test_plan_generator.py
‚îÇ
‚îú‚îÄ‚îÄ Documentation (docs/)
‚îÇ   ‚îú‚îÄ‚îÄ architecture/               # System design, planning
‚îÇ   ‚îú‚îÄ‚îÄ security/                   # OWASP, hooks, git safety
‚îÇ   ‚îú‚îÄ‚îÄ workflow/                   # Install, PR, build, implementation
‚îÇ   ‚îú‚îÄ‚îÄ quality/                    # Quality gates, personas
‚îÇ   ‚îú‚îÄ‚îÄ reference/                  # Compliance, registries
‚îÇ   ‚îî‚îÄ‚îÄ examples/
‚îÇ
‚îî‚îÄ‚îÄ Tooling
    ‚îú‚îÄ‚îÄ Makefile                    # Development tasks
    ‚îú‚îÄ‚îÄ .github/workflows/          # CI/CD
    ‚îú‚îÄ‚îÄ scripts/                    # Utilities
    ‚îÇ   ‚îú‚îÄ‚îÄ benchmark_tokens.py     # Token benchmarks
    ‚îÇ   ‚îî‚îÄ‚îÄ pre-commit.sh           # Pre-commit validation
    ‚îî‚îÄ‚îÄ wfc/cli.py                  # wfc command
```

### Completed Enhancements (v0.1.0)

‚úÖ **wfc-implement Complete** (Phases 1-3):
- Memory system (cross-session learning via ReflexionMemory)
- Confidence-first implementation (prevent wrong-direction work)
- Workflow metrics tracking (tokens, time, success rates)
- Token budget optimization (historical learning)
- Universal quality gate (Trunk.io integration)
- TDD workflow enforcement (RED-GREEN-REFACTOR)
- Merge engine with rollback (main always passing)
- Integration tests (>80% coverage)

### Future State (v0.2.0+)

Planned enhancements:
- Dashboard (WebSocket, Mermaid visualization) - Phase 4 Optional
- Enhanced MCP integration
- Advanced pattern learning
- Distributed execution (cloud agents)

### TEAMCHARTER Governance (v0.1.1+)

**Values-Driven Workflow**: All plans are validated against 6 core values

**6 Core Values**:
1. **Innovation & Experimentation** - Embrace failure as learning, validate through critique
2. **Accountability & Simplicity** - Complexity budgets, Say:Do ratio tracking
3. **Teamwork & Collaboration** - Multi-agent consensus, customer advocate persona
4. **Continuous Learning & Curiosity** - ReflexionMemory, retrospective analysis
5. **Customer Focus & Service Excellence** - Customer-centric interview questions
6. **Trust & Autonomy** - Confidence thresholds, informed decision-making

**Enforcement Mechanisms**:
- **Complexity Budgets**: Pre-gate flags when tasks exceed S/M/L/XL limits
- **Interview Questions**: "Who is the customer?", "What does success look like?"
- **Review Personas**: Customer Advocate ensures stakeholder voice in reviews
- **Memory Tracking**: Values alignment field in reflexion entries
- **Audit Trails**: Immutable proof that validation was performed

**Validated Plan Flow**:

```
Plan Generation ‚Üí Validate (7D critique) ‚Üí Revise ‚Üí Code Review (loop to 8.5+) ‚Üí Final
```

**Governance Documents**:
- `wfc/references/TEAMCHARTER.md` - Human-readable values and enforcement
- `wfc/references/teamcharter_values.json` - Machine-readable schema for agents

**Why This Matters**:
- Prevents over-engineering through evidence-based complexity assessment
- Ensures customer value is central to every task
- Builds institutional memory through values-aligned retrospectives
- Provides accountability through Say:Do ratio tracking

---

## ‚öôÔ∏è Design Principles

### 1. Token Efficiency is Paramount

**Goal**: Minimize token usage while maximizing review quality

**How**:
- **Ultra-minimal prompts**: 200 tokens (was 3000) - 93% reduction
- **File references**: Send paths, not content - 95% reduction
- **Domain guidance**: What to look for, not how to grep
- **Progressive disclosure**: Load only what's needed

**Never**:
- ‚ùå Send full file content to personas
- ‚ùå Use verbose backstories or examples
- ‚ùå Include redundant information
- ‚ùå Exceed token budgets without justification

**Always**:
- ‚úÖ Measure token usage with benchmarks
- ‚úÖ Use file reference architecture
- ‚úÖ Build ultra-minimal prompts
- ‚úÖ Report token savings

### 2. Agent Skills Compliance

**Goal**: All WFC skills comply with Agent Skills specification

**How**:
- Valid frontmatter (only: name, description, license)
- Hyphenated names (wfc-review, not wfc-review)
- Comprehensive descriptions (triggers, use cases, anti-use cases)
- XML prompt generation
- Progressive disclosure pattern
- Validated with skills-ref

**Never**:
- ‚ùå Use colons in skill names
- ‚ùå Include invalid frontmatter fields
- ‚ùå Skip validation before commit
- ‚ùå Break XML prompt generation

**Always**:
- ‚úÖ Validate with `make validate`
- ‚úÖ Test XML generation
- ‚úÖ Follow progressive disclosure
- ‚úÖ Keep SKILL.md < 500 lines

### 3. Evidence-Based Development

**Goal**: Never guess - always verify

**How**:
- Read files before editing
- Check existing code with Glob/Grep
- Verify assumptions with tests
- Measure token usage with benchmarks
- Validate skills with skills-ref

**Never**:
- ‚ùå Implement based on assumptions
- ‚ùå Skip reading existing code
- ‚ùå Guess at file locations
- ‚ùå Trust outdated knowledge

**Always**:
- ‚úÖ Read before writing
- ‚úÖ Search before creating
- ‚úÖ Test before claiming success
- ‚úÖ Benchmark before optimizing

### 4. WFC Philosophy (ELEGANT)

**E**LEGANT: Simplest solution wins
- No over-engineering
- Clear, readable code
- Minimal abstractions

**M**ULTI-TIER: Clear separation of concerns
- Logic separated from presentation
- Personas (logic) vs CLI (presentation)
- Progressive disclosure (load on demand)

**P**ARALLEL: True concurrent execution
- Independent subagents
- No context bleeding
- Claude Code Task tool integration

**P**ROGRESSIVE: Load only what's needed
- SKILL.md first (< 500 lines)
- References on demand
- Scripts when executed

**T**OKEN-AWARE: Every token counts
- Measure with benchmarks
- 99% reduction target
- Budget enforcement

**C**OMPLIANT: Agent Skills spec enforced
- Validated with skills-ref
- XML prompts work
- No regressions

### 5. Quality Over Speed

**Goal**: Correctness and maintainability trump quick implementations

**How**:
- Run `make check-all` before commit
- Fix failing tests immediately
- Document non-obvious decisions
- Follow pre-commit hooks

**Never**:
- ‚ùå Skip tests to save time
- ‚ùå Bypass pre-commit hooks
- ‚ùå Leave TODOs in production code
- ‚ùå Commit broken code

**Always**:
- ‚úÖ Run tests before commit
- ‚úÖ Format code with `make format`
- ‚úÖ Validate skills with `make validate`
- ‚úÖ Check all with `make check-all`

---

## üõ†Ô∏è Implementation Patterns

### TDD Workflow (RED-GREEN-REFACTOR)

**Pattern**: All implementations follow strict TDD workflow

**Phases**:
1. **UNDERSTAND** - Read task, assess confidence (‚â•90%), search past errors
2. **TEST_FIRST** - Write tests BEFORE implementation (RED phase - tests fail)
3. **IMPLEMENT** - Write minimum code to pass tests (GREEN phase - tests pass)
4. **REFACTOR** - Clean up while maintaining passing tests
5. **QUALITY_CHECK** - Run universal quality gate (Trunk.io)
6. **SUBMIT** - Verify all criteria met, route to review

**Why**: Prevents over-engineering, ensures testability, documents behavior

**Example**:
```python
# 1. UNDERSTAND
task = read_task("TASK-001")
confidence = assess_confidence(task)  # Must be ‚â•90%
past_errors = search_similar_errors(task.description)

# 2. TEST_FIRST (RED)
def test_add_logging():
    result = my_function()
    assert "Entered my_function" in captured_logs
    assert "Exited my_function" in captured_logs
# Run tests ‚Üí FAIL (good!)

# 3. IMPLEMENT (GREEN)
def my_function():
    logging.info("Entered my_function")
    # ... implementation ...
    logging.info("Exited my_function")
    return result
# Run tests ‚Üí PASS

# 4. REFACTOR
def my_function():
    log_function_entry("my_function")
    result = _do_work()
    log_function_exit("my_function")
    return result
# Run tests ‚Üí Still PASS

# 5. QUALITY_CHECK
trunk check my_function.py  # Must pass

# 6. SUBMIT
verify_acceptance_criteria(task)
route_to_review(agent_report)
```

### Confidence-First Implementation

**Pattern**: Assess confidence BEFORE starting work (SuperClaude pattern)

**Decision Tree**:
- **‚â•90%**: Proceed with implementation
- **70-89%**: Present alternatives + ask clarifying questions
- **<70%**: STOP - Investigate more, ask user for guidance

**Why**: Prevents 25-250x token waste from wrong-direction work

**Example**:
```python
assessment = confidence_checker.assess(task)

if assessment.confidence_score >= 90:
    proceed_with_implementation()
elif assessment.confidence_score >= 70:
    ask_clarifying_questions(assessment.questions)
    present_alternatives(assessment.alternatives)
else:
    stop_and_investigate(assessment.risks)
```

### Cross-Session Learning (ReflexionMemory)

**Pattern**: Log errors and fixes for future reference

**Files**:
- `wfc/memory/reflexion.jsonl` - Errors and fixes
- `wfc/memory/workflow_metrics.jsonl` - Performance metrics

**Why**: Don't repeat the same mistakes across sessions

**Example**:
```python
# Log reflexion entry
reflexion = ReflexionEntry(
    task_id="TASK-001",
    mistake="Forgot to run tests after refactoring",
    evidence="pytest returned 3 failures",
    fix="Rolled back refactoring commit",
    rule="ALWAYS run tests after refactoring before committing",
    severity="high"
)
memory_manager.log_reflexion(reflexion)

# Before starting new task, search for similar errors
similar = memory_manager.search_similar_errors("refactoring authentication")
if similar:
    warn_about_past_mistakes(similar)
```

### Token Budget Optimization

**Pattern**: Complexity-based budgets with historical learning

**Budgets**:
- S (Simple): 200 tokens
- M (Medium): 1,000 tokens
- L (Large): 2,500 tokens
- XL (Extra Large): 5,000 tokens

**Why**: Prevent over-engineering, optimize based on history

**Example**:
```python
budget = token_manager.create_budget("TASK-M", TaskComplexity.M, use_history=True)
# If history shows M tasks average 1,500 tokens:
# budget.budget_total = 1,500 * 1.2 = 1,800 tokens (20% buffer)

budget = token_manager.update_usage(budget, input_tokens=800, output_tokens=400)
if budget.is_approaching_limit():
    warn("‚ö†Ô∏è APPROACHING BUDGET: 88% used")
```

### Failure Severity Classification

**Pattern**: WARNING (don't block), ERROR (block but retryable), CRITICAL (immediate failure)

**Why**: "Warnings aren't failures but broken code is" (user feedback)

**Decision**:
- **WARNING**: Linting warnings, style issues ‚Üí Don't block
- **ERROR**: Test failures, compilation errors ‚Üí Block but retry (max 2)
- **CRITICAL**: Security vulnerabilities, data loss ‚Üí Immediate failure

**Example**:
```python
severity = classify_test_failure(test_result)

if severity == FailureSeverity.WARNING:
    report_warning_but_continue()
elif severity == FailureSeverity.ERROR:
    if retry_count < max_retries:
        retry_task()
    else:
        fail_with_recovery_plan()
else:  # CRITICAL
    immediate_failure_no_retry()
```

### Merge with Rollback

**Pattern**: Main branch always passing, worktrees preserved on failure

**Workflow**:
1. Rebase on main
2. Re-run tests after rebase
3. Merge to main
4. Run integration tests
5. Rollback if integration tests fail

**Why**: Safety - never break main, preserve evidence for investigation

**Example**:
```python
# Merge workflow
result = merge_engine.merge(task, branch, worktree_path)

if result.status == MergeStatus.SUCCESS:
    cleanup_worktree()
elif result.status == MergeStatus.FAILED_TESTS:
    # Automatic rollback
    git_reset_hard(merge_sha)
    # Preserve worktree for investigation
    result.worktree_preserved = True
    # Re-queue with recovery plan
    if result.should_retry:
        create_recovery_plan(task)
        re_queue_task(task)
```

### Parallel Execution with Dependencies

**Pattern**: Topological sort, respect dependencies, max agents limit

**Why**: Maximum safe concurrency, respect task dependencies

**Example**:
```python
# Task graph with dependencies
tasks = [
    Task("TASK-001", dependencies=[]),
    Task("TASK-002", dependencies=["TASK-001"]),
    Task("TASK-003", dependencies=[]),
]

# Group by dependency level
levels = topological_sort(tasks)
# Level 1: TASK-001, TASK-003 (parallel)
# Level 2: TASK-002 (waits for TASK-001)

# Execute with max_agents limit
for level in levels:
    agents = min(len(level), max_agents)
    execute_parallel(level, agents)
```

---

## üö´ Absolute Rules

### Token Management

**NEVER**:
- Send full file content to personas
- Use verbose persona backstories
- Exceed token budgets without justification
- Skip token benchmarking

**ALWAYS**:
- Use file reference architecture
- Build ultra-minimal prompts (200 tokens)
- Measure token usage with `make benchmark`
- Report token savings to user

### Agent Skills Compliance

**NEVER**:
- Use colons in skill names (use hyphens: `wfc-review`)
- Include invalid frontmatter fields (`user-invocable`, `disable-model-invocation`, `argument-hint`)
- Skip validation (`make validate`)
- Break XML prompt generation

**ALWAYS**:
- Validate with skills-ref before commit
- Use only allowed frontmatter fields (name, description, license)
- Test XML generation
- Keep SKILL.md < 500 lines

### Code Quality

**NEVER**:
- Commit failing tests
- Skip pre-commit hooks
- Bypass linting
- Leave debugging code

**ALWAYS**:
- Run `make check-all` before commit
- Format code with `make format`
- Fix linting errors
- Update tests when changing code

### Development Workflow

**NEVER**:
- Use `python -m` or `pip install` directly
- Bypass Make for common tasks
- Commit without running tests
- Skip documentation updates

**ALWAYS**:
- Use UV for Python operations (`uv run pytest`)
- Use Make for common tasks (`make test`, `make validate`)
- Run `make check-all` before commit
- Update docs when changing functionality

### Git Workflow

**NEVER**:
- Force push to main/master
- Commit secrets or credentials
- Skip commit message quality
- Bypass CI/CD checks

**ALWAYS**:
- Write clear commit messages
- Include Co-Authored-By for AI assistance
- Let CI/CD run before merge
- Follow conventional commits (optional)

---

## üéØ Quality Gates

### Pre-commit

**Must pass before commit**:
1. ‚úÖ All skills validate (skills-ref)
2. ‚úÖ All tests pass (pytest)
3. ‚úÖ Code is formatted (black + ruff)
4. ‚úÖ No linting errors (ruff)

**Command**: `make check-all`

### CI/CD

**Must pass before merge**:
1. ‚úÖ All tests pass
2. ‚úÖ All skills validate
3. ‚úÖ Code formatting correct
4. ‚úÖ No linting errors
5. ‚úÖ XML prompts generate correctly
6. ‚úÖ Token benchmarks run

**Workflow**: `.github/workflows/validate.yml`

### Release

**Must pass before release**:
1. ‚úÖ All quality gates pass
2. ‚úÖ Documentation updated
3. ‚úÖ CHANGELOG.md updated
4. ‚úÖ Version bumped in pyproject.toml
5. ‚úÖ Token benchmarks show 99% reduction

---

## üìä Key Metrics

### Token Reduction

**Target**: 99% reduction
**Current**: 99% (150k ‚Üí 1.5k tokens)

**Measurement**: `make benchmark`

### Agent Skills Compliance

**Target**: 100% skills validated
**Current**: 17/17 (100%)

**Measurement**: `make validate`

### Test Coverage

**Target**: >80%
**Current**: TBD (run `make test-coverage`)

**Measurement**: `pytest --cov=wfc`

### Code Quality

**Target**: No linting errors
**Current**: 0 errors

**Measurement**: `make lint`

---

## üîÑ Development Workflow

### Feature Development

1. **Create branch**: `git checkout -b feature/name`
2. **Develop**: Make changes, write tests
3. **Check**: `make check-all`
4. **Commit**: Clear message with Co-Authored-By
5. **Push**: Let CI/CD run
6. **Merge**: After CI/CD passes

### Bug Fixes

1. **Write test**: Reproduce bug
2. **Fix**: Minimum necessary change
3. **Verify**: Test passes, `make check-all`
4. **Commit**: Reference issue number
5. **Merge**: After CI/CD passes

### Documentation

1. **Update**: Keep docs in sync with code
2. **Examples**: Add usage examples
3. **Validation**: Run `make validate` if changing skills
4. **Commit**: Document changes in commit message

---

## üöÄ Release Process

### Version Bumping

**Semantic Versioning**: MAJOR.MINOR.PATCH

- **MAJOR**: Breaking changes
- **MINOR**: New features (backward compatible)
- **PATCH**: Bug fixes (backward compatible)

### Release Checklist

1. ‚úÖ All quality gates pass
2. ‚úÖ CHANGELOG.md updated
3. ‚úÖ Version bumped in pyproject.toml
4. ‚úÖ Documentation updated
5. ‚úÖ Token benchmarks run
6. ‚úÖ All skills validated
7. ‚úÖ Git tag created
8. ‚úÖ PyPI package published (future)

---

## üìö References

### Documentation

- **CLAUDE.md** - Session guidance for Claude Code
- **QUICKSTART.md** - Get started in 5 minutes
- **CONTRIBUTING.md** - How to contribute
- **docs/reference/AGENT_SKILLS_COMPLIANCE.md** - Compliance details

### Code

- **wfc/scripts/personas/** - Persona system
- **wfc/scripts/skills/** - Skill implementations
- **wfc/references/** - Progressive disclosure docs
- **~/.claude/skills/wfc-*/** - Installed skills

### Validation

- **skills-ref**: ~/repos/agentskills/skills-ref
- **Command**: `make validate`

---

**This is World Fucking Class.** üöÄ
